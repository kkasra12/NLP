<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="markdown.css" type="text/css" />
</head>
<body>
<h1 id="tokenizer">Tokenizer</h1>
<h2 id="regex">Regex</h2>
<p>The tokenizer uses this regexes to find the tokens:</p>
<table style="width:42%;">
<colgroup>
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">TokenLabel</th>
<th align="left">Regex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">PUNCTUATION&quot;</td>
<td align="left"><code>r'[!&quot;#$%&amp;\\\'()*+,\\-./:;â‰ˆâ—ï¸,Â«Â»&lt;=&gt;?@\\[\\\\\\]\\^_`\|~ØŒâ€â€œ.Ã—â‹…â€²âˆ’â€“Ø›Ø›ØŸ{}â€¦]'</code></td>
</tr>
<tr class="even">
<td align="left">&quot;EMOJI&quot;</td>
<td align="left"><code>&quot;[&quot;+open(join(DIRNAME,&quot;./emojies&quot;)).read().replace(&quot;\n&quot;,&quot;&quot;)+&quot;]&quot;</code></td>
</tr>
<tr class="odd">
<td align="left">&quot;IP&quot;</td>
<td align="left"><code>r&quot;\d\.\d\.\d\.\d&quot;</code></td>
</tr>
<tr class="even">
<td align="left">&quot;NUMBER&quot;</td>
<td align="left"><code>r&quot;(?:+(?:(?:.+)</td>
</tr>
<tr class="odd">
<td align="left">&quot;PERSIAN_WORD&quot;</td>
<td align="left"><code>f&quot;[{persianLetters}][{persianLetters}\u200c]*[{persianLetters}]|[{persianLetters}]&quot;</code></td>
</tr>
<tr class="even">
<td align="left">&quot;ENGLISH_WORD&quot;</td>
<td align="left"><code>r&quot;[A-Za-z]+(?:\'[a-z]+)?&quot;</code></td>
</tr>
<tr class="odd">
<td align="left">&quot;SENTENCE_DELIMITERS&quot;</td>
<td align="left"><code>r&quot;[.?!ØŸ]&quot;</code></td>
</tr>
<tr class="even">
<td align="left">&quot;ENGLISH_HUMAN_NAME&quot;</td>
<td align="left"><code>r&quot;(?:(?i)mr\|mis\|miss)\.[A-Za-z][a-z]+&quot;</code></td>
</tr>
<tr class="odd">
<td align="left">&quot;EMAIL&quot;</td>
<td align="left"><pre style="border-radius:20px;"><code>r&quot;[a-zA-Z0-9.!#$%&amp;'*+\/=?^_{\|}~-]+
  @
  (?:
    [a-zA-Z0-9]
    [-a-zA-Z0-9%_\+~#=]*
    \.
  )+
  [a-zA-Z]
  [a-zA-Z0-9]{0,6}&quot;</code></pre></td>
</tr>
<tr class="even">
<td align="left">&quot;LINK&quot;</td>
<td align="left"><pre style="border-radius:20px;"><code>r&quot;(?:https?:\/\/)?
  (?:www\d?\.)?
  (?:
    [a-zA-Z0-9]
    [-a-zA-Z0-9%_\+~#=]*
    \.
  )+
  [a-zA-Z]
  [a-zA-Z0-9]{0,6}
  (?:
    /
    [a-zA-Z0-9_%+=~\-#@]+
    (?:
      \.
      [a-zA-Z0-9_%+=~\-@]+
    )*
  )*
  \/?
  (?:
    \?
    (?:
      [a-zA-Z]
      [a-zA-Z0-9]*
      =
      [a-zA-Z0-9+\.\-%]*
      &amp;
    )*
    (?:
      [a-zA-Z]
      [a-zA-Z0-9]*
      =
      [a-zA-Z0-9+\.\-%]*
    )
  )?&quot;</code></pre></td>
</tr>
<tr class="odd">
<td align="left">&quot;GREEL_LETTERS&quot;</td>
<td align="left"><code>&quot;[Î‘Î±Î’Î²Î“Î³Å‹Î”Î´Î•ÎµÎ–Î¶Î—Î·Î˜Î¸Î™Î¹ÎšÎºÎ›Î»ÎœÎ¼ÎÎ½ÎÎ¾ÎŸÎ¿Î Ï€Î¡ÏÎ£ÏƒÎ¤Ï„Î¥Ï…Î¦Ï†Î§Ï‡Î¨ÏˆÎ©Ï‰âˆ‘âˆˆ]&quot;</code></td>
</tr>
<tr class="even">
<td align="left">&quot;DEGREE&quot;</td>
<td align="left"><code>r&quot;(\d+Â°C)\|(\d+Â°F)&quot;</code></td>
</tr>
<tr class="odd">
<td align="left">&quot;LEFT_TO_RIGHT_MARK &quot;</td>
<td align="left"><code>&quot;\u200e&quot;</code></td>
</tr>
<tr class="odd">
<td align="left">&quot;RIGHT_TO_LEFT_MARK  &quot;</td>
<td align="left"><code>&quot;\u200f&quot;</code></td>
</tr>
<tr class="even">
<td align="left">&quot;NEW_LINE&quot;</td>
<td align="left"><code>r&quot;\n&quot;</code></td>
</tr>
<tr class="odd">
<td align="left">&quot;SPACE&quot;</td>
<td align="left"><code>&quot; &quot;</code></td>
</tr>
<tr class="even">
<td align="left">&quot;TAB&quot;</td>
<td align="left"><code>r&quot;\t&quot;</code></td>
</tr>
</tbody>
</table>
<br>
<blockquote>
<p>emojies are stored in a file named <code>emojies</code> which is available in <a href="./pltk/Tokenizer/emojies" class="uri">./pltk/Tokenizer/emojies</a></p>
</blockquote>
<blockquote>
<p>persianLetters is a variable which contains all of the persian letters</p>
</blockquote>
<blockquote>
<p>DOT OPERATOR' (U+22C5) ==&gt; â‹… ,PRIME (U+2032) ==&gt; â€² ,MINUS SIGN (U+2212) ==&gt; âˆ’ ,EN DASH (U+2013) ==&gt; â€“ ,ARABIC SEMICOLON' (U+061B) ,arabic question mark (U+061F) ,FUNCTION APPLICATION (U+2061) are also supported as <code>PUNCTUATION</code></p>
</blockquote>
<h2 id="normalizer">Normalizer</h2>
<p>the normalizer conversions are described bellow:<br> <strong>The characters in column A converts to characters in column B</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">A</th>
<th align="left">B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Ù¡</td>
<td align="left">Û±</td>
</tr>
<tr class="even">
<td align="left">Ù¢</td>
<td align="left">Û²</td>
</tr>
<tr class="odd">
<td align="left">Ù£</td>
<td align="left">Û³</td>
</tr>
<tr class="even">
<td align="left">Ù¤</td>
<td align="left">Û´</td>
</tr>
<tr class="odd">
<td align="left">Ù¥</td>
<td align="left">Ûµ</td>
</tr>
<tr class="even">
<td align="left">Ù¦</td>
<td align="left">Û¶</td>
</tr>
<tr class="odd">
<td align="left">Ù§</td>
<td align="left">Û·</td>
</tr>
<tr class="even">
<td align="left">Ù¨</td>
<td align="left">Û¸</td>
</tr>
<tr class="odd">
<td align="left">Ù©</td>
<td align="left">Û¹</td>
</tr>
<tr class="even">
<td align="left">Ùª</td>
<td align="left">%</td>
</tr>
<tr class="odd">
<td align="left">Ù«</td>
<td align="left">.</td>
</tr>
<tr class="even">
<td align="left">Ù¬</td>
<td align="left">Ù¬</td>
</tr>
<tr class="odd">
<td align="left">Ù­</td>
<td align="left">*</td>
</tr>
<tr class="even">
<td align="left">Ù¯Ú¨Ú§</td>
<td align="left">ï»•</td>
</tr>
<tr class="odd">
<td align="left">ïºÙ±Ù³ÙµÙ²Ø£Ø¥ïº</td>
<td align="left">Ø§</td>
</tr>
<tr class="even">
<td align="left">ïº</td>
<td align="left">Ø¢</td>
</tr>
<tr class="odd">
<td align="left">ï»­ï»®ÛŠÛ‰Û‹Û†Û®ÛˆÛ…Û‡Û„Ù¶ÛÙ·</td>
<td align="left">ï»­</td>
</tr>
<tr class="even">
<td align="left">Ù¹Ù¿</td>
<td align="left">ïº™</td>
</tr>
<tr class="odd">
<td align="left">ïº’ïºïº‘</td>
<td align="left">Ø¨</td>
</tr>
<tr class="even">
<td align="left">ïº˜ïº•ïº–ïº—Ù¼ÙºÙ½</td>
<td align="left">ïº•</td>
</tr>
<tr class="odd">
<td align="left">Ú€ï­–</td>
<td align="left">ï­–</td>
</tr>
<tr class="even">
<td align="left">Ú</td>
<td align="left">ïº¡</td>
</tr>
<tr class="odd">
<td align="left">ïº§Ú…Ú‚Ú¿ïº¥</td>
<td align="left">Ø®</td>
</tr>
<tr class="even">
<td align="left">Û¾ï­ºÚƒ</td>
<td align="left">ï­º</td>
</tr>
<tr class="odd">
<td align="left">ïºïº Ú„Ú‡</td>
<td align="left">ïº</td>
</tr>
<tr class="even">
<td align="left">ïº¢ïº£</td>
<td align="left">Ø­</td>
</tr>
<tr class="odd">
<td align="left">ïºªïº©ÚˆÚ‰ÚÚŠ</td>
<td align="left">ïº©</td>
</tr>
<tr class="even">
<td align="left">ÚŒÚÚ‹ÚÚ</td>
<td align="left">ïº«</td>
</tr>
<tr class="odd">
<td align="left">Ú™Ú’ï®ŠÚ‘</td>
<td align="left">ï®Š</td>
</tr>
<tr class="even">
<td align="left">ïº®ïº­Ú–Ú•Ú”Ú“</td>
<td align="left">ïº­</td>
</tr>
<tr class="odd">
<td align="left">ïº¯ïº°Û¯Ú—</td>
<td align="left">ïº¯</td>
</tr>
<tr class="even">
<td align="left">ïº±ïº³ïº´ÚšÚ›</td>
<td align="left">ïº±</td>
</tr>
<tr class="odd">
<td align="left">ïº·ïº¸ÛºÚœïºµ</td>
<td align="left">Ø´</td>
</tr>
<tr class="even">
<td align="left">ïº¹ïº»ÚÛ»</td>
<td align="left">ïº¹</td>
</tr>
<tr class="odd">
<td align="left">Ú</td>
<td align="left">ïº½</td>
</tr>
<tr class="even">
<td align="left">ÚŸ</td>
<td align="left">ï»</td>
</tr>
<tr class="odd">
<td align="left">ï»ï»Ú Û¼</td>
<td align="left">ï»</td>
</tr>
<tr class="even">
<td align="left">ï»‹ï»Œ</td>
<td align="left">Ø¹</td>
</tr>
<tr class="odd">
<td align="left">ï»‘ï»“Ú£Ú¦Ú¡Ú¢Ú¥Ú¤</td>
<td align="left">ï»‘</td>
</tr>
<tr class="even">
<td align="left">ï»•</td>
<td align="left">Ù‚</td>
</tr>
<tr class="odd">
<td align="left">ï®Ø»Ú«Ø¼Ú®Ú­ÚªÚ¬ï®</td>
<td align="left">Ú©</td>
</tr>
<tr class="even">
<td align="left">ï®”Ú´Ú°Ú²ï®’Ú±Ú³</td>
<td align="left">ï®’</td>
</tr>
<tr class="odd">
<td align="left">ï»ï»ï» Ú¸Ú·Ú¶Úµ</td>
<td align="left">ï»</td>
</tr>
<tr class="even">
<td align="left">ï»§ï»¨ï»¥Ú¹Ú»ÚºÚ¼Ú½</td>
<td align="left">ï»¥</td>
</tr>
<tr class="odd">
<td align="left">ï»„</td>
<td align="left">Ø·</td>
</tr>
<tr class="even">
<td align="left">ï»¬ï»ªï»«Û•Û‚ÛÛ€ÛƒÚ¾Û¿ï»©</td>
<td align="left">Ù‡</td>
</tr>
<tr class="odd">
<td align="left">Û½</td>
<td align="left">ï»‰</td>
</tr>
<tr class="even">
<td align="left">ï»¤ï»¢ï»£</td>
<td align="left">Ù…</td>
</tr>
<tr class="odd">
<td align="left">ÛœÛ©Û¡Û–Û¬Û£Û¢ÛÛ ÛšÛ¥Û¤Û›ÛÛ§Û˜Û¦Û­Û—Û«Û¨ÛŸÛªÛ™</td>
<td align="left">_</td>
</tr>
<tr class="even">
<td align="left">ï¯¾ï¯½ï¯¿ÛŒÛÛÛ’Û“ÛØ½ï»±Ø Û‘Ø¾Ø¿Ù¸ï»¯ïº‰ï¯¼ÙŠØ¦Ù‰</td>
<td align="left">ÛŒ</td>
</tr>
<tr class="odd">
<td align="left">â€”Ù€</td>
<td align="left">_</td>
</tr>
<tr class="even">
<td align="left">ØŒ</td>
<td align="left">,</td>
</tr>
<tr class="odd">
<td align="left">&quot;\u2063&quot;<br>(INVISIBLE SEPARATOR)</td>
<td align="left">,</td>
</tr>
<tr class="even">
<td align="left">&quot;\u2067&quot;<br>(RIGHT-TO-LEFT ISOLATE)</td>
<td align="left">&quot;\u200f&quot;<br>(RIGHT_TO_LEFT_MARK)</td>
</tr>
<tr class="odd">
<td align="left">'\u200d'</td>
<td align="left">'\u200c'</td>
</tr>
<tr class="even">
<td align="left">'\ufeff'</td>
<td align="left">'\u200c'</td>
</tr>
<tr class="odd">
<td align="left">'\u00a0'</td>
<td align="left">'\u200c'</td>
</tr>
</tbody>
</table>
<br>
<br>
<br>
<br>
<h2 id="coding">Coding</h2>
<p>The tokenizer function is available in <code>pltk.Tokenizer.wordTokenizer</code>.<br><br><br> <strong>Arguments:</strong><br></p>
<table style="width:42%;">
<colgroup>
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Arguments</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">text</td>
<td align="left"><code>string</code><br> a text to tokenize</td>
</tr>
<tr class="even">
<td align="left">tokensMap</td>
<td align="left"><code>dict</code> <em>optional</em><br>A dictionary in which <code>Tokens Labels</code> point to <code>compiled regex patterns</code>(e.g. <code>re.compile</code> output)<br><strong>default token map is described above</strong></td>
</tr>
<tr class="odd">
<td align="left">Normalizer</td>
<td align="left"><code>function</code><em>optional</em><br>a function used for normalizing the text prior to tokenizing<br><strong>default normalizer is described above</strong></td>
</tr>
<tr class="even">
<td align="left">verbose</td>
<td align="left"><code>bool</code><em>optional</em><br>shows a progress bar if <code>True</code></td>
</tr>
</tbody>
</table>
<p><br><strong>Returns:</strong><br> Returns two lists of <code>Token</code> class as detected tokens and characters which does not fit in any of the known regexes.</p>
<h2 id="testing-tokenizer-on-telegram-data">Testing Tokenizer on Telegram Data</h2>
<p>The data extracted from telegram channels contains 21217 lines and 5258018 characters(this numbers are reported by <a href="https://linux.die.net/man/1/wc">wc command</a>)</p>
<p>the <code>runme.py</code> program tokenize the the these data and counts the frequency of each token.All tokens are stored in <code>tokensCount.xlsx</code> file and the output is:</p>
<pre><code>$ python3 runme.py
wordTokenizer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21218/21218 [00:35&lt;00:00, 597.51it/s]

wordCounter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1976270/1976270 [00:03&lt;00:00, 622662.52it/s]
tokenizerFinished...
                                                    type text  termFrequency
&lt;TOKEN type:SPACE pos:(4, 5)&gt;,[ ]                  SPACE              851769
&lt;TOKEN type:PUNCTUATION pos:(191, 192)&gt;,[!]  PUNCTUATION    !           2569
&lt;TOKEN type:PUNCTUATION pos:(35, 36)&gt;,[&quot;]    PUNCTUATION    &quot;           2200
&lt;TOKEN type:PUNCTUATION pos:(3, 4)&gt;,[#]      PUNCTUATION    #           3509
&lt;TOKEN type:PUNCTUATION pos:(238, 239)&gt;,[%]  PUNCTUATION    %             96
...                                                  ...  ...            ...
&lt;TOKEN type:EMOJI pos:(0, 1)&gt;,[ğŸ¤¼]                  EMOJI    ğŸ¤¼              1
&lt;TOKEN type:EMOJI pos:(943, 944)&gt;,[ğŸ¥€]              EMOJI    ğŸ¥€              1
&lt;TOKEN type:EMOJI pos:(85, 86)&gt;,[ğŸ¥‡]                EMOJI    ğŸ¥‡              3
&lt;TOKEN type:EMOJI pos:(93, 94)&gt;,[ğŸ¥ˆ]                EMOJI    ğŸ¥ˆ              3
&lt;TOKEN type:EMOJI pos:(103, 104)&gt;,[ğŸ¥‰]              EMOJI    ğŸ¥‰              3

[33763 rows x 3 columns]
</code></pre>
<p>As you can see the output contains 33763 unique tokens also the labels and term frequencies are specified in `tokensCount.xlsx`.</p>
<p>The program used 35 seconds for the tokenizing and 3 seconds for counting the tokens frequency.</p>
<h1 id="calculating-tf-idf">calculating TF-IDF</h1>
<h2 id="codes">Codes</h2>
<h3 id="tokenizer-1">Tokenizer</h3>
<p>the tokenizer is descriped in last project</p>
<h3 id="tfdocumentsoutputformatsparsematrix">TF(documents,outputFormat='sparseMatrix')</h3>
<ul>
<li><strong>Parameters:</strong> documents: string, list an string contains a folder name of corpus or list of documents outputFormat: string 'sparseMatrix' or 'pandas_DataFrame'</li>
<li><strong>Returns:</strong><br> an sparse matrix or <code>pandas.DataFrame</code> object of term frequency calculated.</li>
</ul>
<p>In order to calculate the TF of the documents, several steps are taken as descriped below: - Fistly, each file will be opened and read using <code>open</code> module. - Secondly, the data extracted from files are passed to <code>tokenizer.wordTokenizer</code> which is implemented in the previous assignment.this step will convert the text into the list of tokens. - In the third step,the list of tokens are passed to <code>tokenizer.wordCounter</code> in order to count the frequency of each token in the current document. This finction is also implemented in the previous assignment. The output will be a <code>dict</code> object which maps tokens into their frequency. - As the next step, The last calculated dictionary is stored as the value of another dictionary. This nested dictionary structures is forming a sparse matrix which saves more space than regular matrix. - Finally, the function will return the output matrix as <code>pandas.DataFrame</code> if needed.</p>
<h3 id="dfdocumentsoutputformatsparsematrix">DF(documents,outputFormat='sparseMatrix')</h3>
<ul>
<li><p><strong>Parameters:</strong> documents: string, list an string contains a folder name of corpus or list of documents outputFormat: string 'sparseMatrix' or 'pandas_DataFrame'</p></li>
<li><p><strong>Returns:</strong> <br> map of tokens to their DocumentFrequency of <code>pandas.DataFrame</code></p></li>
</ul>
<p>In order to calculate the DF of the documents, several steps are taken as descriped below: - Fistly, each file will be opened and read using <code>open</code> module. - Secondly, the data extracted from files are passed to <code>tokenizer.wordTokenizer</code> which is implemented in the previous assignment.this step will convert the text into the list of tokens. - In the third step, the frequency of each token will increase by one value if that token is in the current document. - Finally, the function will return the output matrix as <code>pandas.DataFrame</code> if needed.</p>
<h3 id="tf_idftf_matdf_matoutputformatsparsematrix">TF_IDF(TF_mat,DF_mat,outputFormat='sparseMatrix')</h3>
<ul>
<li><p>Parameters:</p>
<pre><code>TF_mat:
the output of TF function
DF_mat:
the output of DF or DF_fromTF function
outputFormat: string
&#39;sparseMatrix&#39; or &#39;pandas_DataFrame&#39;</code></pre></li>
<li><p>Returns: <br> an sparse matrix or <code>pandas.DataFrame</code> object of TF-IDF calculated.</p></li>
</ul>
<p>Having TF and DF will makes calculation of the TF-IDF pretty easy. simply each row of the TF matrix is divided by the corresponding term in the DF matrix.</p>
<h3 id="calculate-df-matrix-using-tf-matrix">calculate DF matrix using TF matrix</h3>
<p>ther is another faster way to calculate DF matrix and that is using TF matrix.To do so,number of non-zero values will be count in each line of the TF matrix. This operation is incredibly fast using <code>pandas.DataFrame</code>: - First: divide the TF matrix by itself - Second: add numbers in each line and save the answer as the DocumentFrequency of terms.</p>
<h2 id="test">Test</h2>
<p>There is two test corpus available: - CorpusSmall: contains two small files types by my self just to test the outputs. - CorpusBig: contains 150 file which each files has 16 lines.</p>
<h2 id="output">Output</h2>
<p>there is brief information in standard output containing execution time and small view of the matrixes.</p>
<h3 id="execution-time">Execution Time:</h3>
<pre><code>The TF matrix is calculated in 4.823282718658447

The DF matrix is calculated in 0.007876873016357422

The TF-IDF matrix is calculated in 0.15381646156311035</code></pre>
<p>obviously calculating the TF requires the hard drive file access, which makes it slowly.</p>
<h3 id="saved-files">Saved Files</h3>
<p>There is three saved files <code>tf.xlsx</code>, <code>df.xlsx</code>, <code>tf_idf.xlsx</code>.(their names are clear enough to decribe their contents)</p>
<h4 id="tf.xlsx">tf.xlsx</h4>
<p>5045 rows x 150 columns rows contains tokens. columns contains documents.</p>
<h4 id="df.xlsx">DF.xlsx</h4>
<p>5046 tokens are found and sorted.</p>
<h4 id="tf_idf.xlsx">tf_idf.xlsx</h4>
<p>5045 rows x 150 columns rows contains tokens. columns contains documents.</p>
</body>
</html>
