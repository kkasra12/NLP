\documentclass[]{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}


\author{Kasra Eskandari\\955361005}
\title{Hidden Markove Model}
\begin{document}

  \maketitle

  \section{Data Structure}\label{data-structure}

  the data structure must contain tokens and part of speech (PoS), in
  order each token must be in a single line, in which tokens and PoS must
  be separated with a space character.

  \section{Preprocess}\label{preprocess}

  this method is implemented in \texttt{HMM} class available in
  \texttt{pltk/HMM/hmm.py} file. this method will convert the data file
  into a list of tuples, un which each tuple contains the token and its
  PoS.

  \begin{quote}
    NOTE: each token will be normalized.
  \end{quote}

  \begin{quote}
    NOTE: The lines having nonstandard structure will be ignored
  \end{quote}

  \section{Train}\label{train}

  to train the model we have to populate two matrix:

  \begin{itemize}
    % \tightlist
    \item
      \(stateTransision_{N\times N}\) :
      \(stateTransision[i,j]=P(State_j|State_i)\) WHERE \(N\) is the number
      of states.
    \item
      \(tokenProbability_{N\times M}\) :
      \(tokenProbability[i,j]=P(Token_j|State_i)\) WHERE \(M\) is the number
      of tokens.
  \end{itemize}

  \begin{quote}
    NOTE: To avoid underflow we will use the logarithm value of
    probabilities
  \end{quote}

  The pseudo code of calculation is described below:

  \begin{itemize}
    % \tightlist
    \item
      count the tokens for each state, also number of altered states and
      record them in appropriate matrix
    \item
      find the number of all states(name is \(N\))
    \item
      calculate logarithm value of both matrices
    \item
      consider the fact that \(log(\frac{a}{b})=log(a)-log(b)\) we can avoid
      deviding small numbers
  \end{itemize}

  \textbf{The algorithm's complication} is \(O ( N\times M)\)

  \section{Finding The Most Probable State
  Sequence}\label{finding-the-most-probable-state-sequence}

  consider state sequence as \(S_0S_1S_2S_3\) for tokens of
  \(T=t_0t_1t_2t_3\), we have: \[
  P( T) =P( S_{0} |\$)\prod ^{3}_{1} P( S_{i} |S_{i-1}) * P( t_{i} |S_{i}) =e^{log( P( T))}
  \] \[
  log( P( T)) =P( S_{0} |\$)\sum ^{3}_{1} P( S_{i} |S_{i-1}) +P( t_{i} |S_{i})
  \]

  as we know \(f(x)=e^x\) is a ascending function, which concludes from,
  if and only if we maximize \(log(P(T))\) the \(P(T)\) will maximize too.

  Too maximize the \(log(P(T))\) we can localy focus on each \[
  P( S_{i} |S_{i-1}) +P( t_{i} |S_{i})
  \] term, which has \(N\) conditions for each token(\(N\) is the number
  of states).

  In order too find this value(and also corresponding state sequence), we
  will form a matrix in which its columns name are tokens and rows name
  indicates the states.In the next step, for each cell(\(S_i\), \(t_j\))
  we will sum two arrays index by index,one taken from \(stateTransision\)
  matrix's \(i\)'s row, and the other taken from \(tokenProbability\)
  matrix's \(j\)'s column. At last, we will find and replace the maximum
  value for whole array with the previous column.

  \textbf{The algorithm's complication} is \(O ( N^2\times M)\)

\end{document}
